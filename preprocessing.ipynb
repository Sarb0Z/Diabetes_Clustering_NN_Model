{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def fix_age(age) -> np.int8:\n",
    "    if age == '[0-10)':\n",
    "        return 5\n",
    "    elif age == '[10-20)':\n",
    "        return 15\n",
    "    elif age == '[20-30)':\n",
    "        return 25\n",
    "    elif age == '[30-40)':\n",
    "        return 35\n",
    "    elif age == '[40-50)':\n",
    "        return 45\n",
    "    elif age == '[50-60)':\n",
    "        return 55\n",
    "    elif age == '[60-70)':\n",
    "        return 65\n",
    "    elif age == '[70-80)':\n",
    "        return 75\n",
    "    elif age == '[80-90)':\n",
    "        return 85\n",
    "    elif age == '[90-100)':\n",
    "        return 95\n",
    "\n",
    "\n",
    "def fix_demographics(row) -> np.int8:\n",
    "    if row['gender'] == 'Female':\n",
    "        if row['race'] == 'Caucasian':\n",
    "            return 1\n",
    "        elif row['race'] == 'Asian':\n",
    "            return 2\n",
    "        elif row['race'] == ' AfricanAmerican':\n",
    "            return 3\n",
    "        elif row['race'] == 'Hispanic':\n",
    "            return 4\n",
    "        elif row['race'] == 'Other':\n",
    "            return 5\n",
    "        else:\n",
    "            return 6\n",
    "    elif row['gender'] == 'Male':\n",
    "        if row['race'] == 'Caucasian':\n",
    "            return 7\n",
    "        elif row['race'] == 'Asian':\n",
    "            return 8\n",
    "        elif row['race'] == ' AfricanAmerican':\n",
    "            return 9\n",
    "        elif row['race'] == 'Hispanic':\n",
    "            return 10\n",
    "        elif row['race'] == 'Other':\n",
    "            return 11\n",
    "        else:\n",
    "            return 12\n",
    "    else:\n",
    "        if row['race'] == 'Caucasian':\n",
    "            return 13\n",
    "        elif row['race'] == 'Asian':\n",
    "            return 14\n",
    "        elif row['race'] == ' AfricanAmerican':\n",
    "            return 15\n",
    "        elif row['race'] == 'Hispanic':\n",
    "            return 16\n",
    "        elif row['race'] == 'Other':\n",
    "            return 17\n",
    "        else:\n",
    "            return 18\n",
    "\n",
    "\n",
    "def data_normalization(item) -> np.int8:\n",
    "    # Values: '>200,' '>300','normal', and 'none' if not measured\n",
    "    if item == 'None' or item == 'No' or item == 'NO':\n",
    "        return 0\n",
    "    elif item == 'Yes' or item == 'Ch' or item == 'Steady' or item == '<30' or item == 'Norm':\n",
    "        return 1\n",
    "    elif item == '>30' or item == 'Up' or item == '>200' or item == '>7':\n",
    "        return 2\n",
    "    elif item == '>300' or item == 'Down' or item == '>8':\n",
    "        return 3\n",
    "    elif item == '?':\n",
    "        return None\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # read file to dataframe\n",
    "    df = pd.read_csv(\"diabetic_data.csv\")\n",
    "\n",
    "    # remove data that is irrelevant\n",
    "    df = df.drop(['encounter_id', 'weight', 'patient_nbr', 'discharge_disposition_id', 'admission_source_id',\n",
    "                'payer_code', 'number_outpatient', 'number_emergency', 'number_inpatient', 'medical_specialty'], axis=1)\n",
    "   \n",
    "    # prepare data for a Classification or Clustering machine learning \n",
    "    # algorithm by converting data to numeric or nominal format\n",
    "    df['age'] = df['age'].apply(fix_age)\n",
    "    df['demographics'] = df.apply(fix_demographics, axis=1)\n",
    "    df = df.applymap(lambda item: data_normalization(item))\n",
    "    # df['max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide,glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone', 'change', 'diabetesMed', 'readmitted'] = df['max_glu_serum',\n",
    "\n",
    "    # df.info()\n",
    "    # df.dropna(axis=1, thresh=10)\n",
    "    # remove data that has null values\n",
    "    df  = df.drop(['gender', 'race'], axis=1)\n",
    "    df.dropna(axis=0)\n",
    "\n",
    "    # divide 80/20 for training and testing\n",
    "    df1 = df[df.index % 5 != 0]  # Excludes every 5th row starting from 0\n",
    "    df2 = df[df.index % 5 == 0]  # Selects every 5th raw starting from 0\n",
    "\n",
    "    \n",
    "    df1.to_csv(\"diabetic_data_training.csv\")\n",
    "    df2.to_csv(\"diabetic_data_testing.csv\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "tensor([1, 2, 3]) cpu\n",
      "tensor([1, 2, 3], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# x = torch.rand(5, 3)\n",
    "# print(x)\n",
    "# !nvidia-smi\n",
    "torch.cuda.is_available()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "# Count number of devices\n",
    "torch.cuda.device_count()\n",
    "\n",
    "# Create tensor (default on CPU)\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Tensor not on GPU\n",
    "print(tensor, tensor.device)\n",
    "\n",
    "# Move tensor to GPU (if available)\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "print(tensor_on_gpu)\n",
    "\n",
    "# If tensor is on GPU, can't transform it to NumPy (this will error)\n",
    "# tensor_on_gpu.numpy()\n",
    "\n",
    "# Instead, copy the tensor back to cpu\n",
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarvz\\AppData\\Local\\Temp\\ipykernel_11992\\3525404029.py:152: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = train_df.append(test_df.drop(columns=['ID']))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (81412, 51) orignal:  (81412, 51)\n",
      "test shape:  (20354, 51) original:  (20354, 51)\n",
      "Counter({'NO': 43877, '>30': 28461, '<30': 9074})\n",
      "Counter({2: 43877, 1: 28461, 0: 9074})\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .cat accessor with a 'category' dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 247\u001b[0m\n\u001b[0;32m    243\u001b[0m     sample_df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mdiabetic_trained_model_probabilties.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 247\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[5], line 194\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    190\u001b[0m X_train\u001b[39m.\u001b[39mhead()\n\u001b[0;32m    192\u001b[0m \u001b[39m# Choosing columns for embedding\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[39m#categorical embedding for columns having more than three values\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m embedded_cols \u001b[39m=\u001b[39m {n: \u001b[39mlen\u001b[39m(col\u001b[39m.\u001b[39mcat\u001b[39m.\u001b[39mcategories) \u001b[39mfor\u001b[39;00m n,col \u001b[39min\u001b[39;00m df1\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(col\u001b[39m.\u001b[39mcat\u001b[39m.\u001b[39mcategories) \u001b[39m>\u001b[39m \u001b[39m3\u001b[39m}\n\u001b[0;32m    195\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEmbedded columns:\u001b[39m\u001b[39m\"\u001b[39m, embedded_cols)\n\u001b[0;32m    197\u001b[0m embedded_col_names \u001b[39m=\u001b[39m embedded_cols\u001b[39m.\u001b[39mkeys()\n",
      "Cell \u001b[1;32mIn[5], line 194\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    190\u001b[0m X_train\u001b[39m.\u001b[39mhead()\n\u001b[0;32m    192\u001b[0m \u001b[39m# Choosing columns for embedding\u001b[39;00m\n\u001b[0;32m    193\u001b[0m \u001b[39m#categorical embedding for columns having more than three values\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m embedded_cols \u001b[39m=\u001b[39m {n: \u001b[39mlen\u001b[39m(col\u001b[39m.\u001b[39mcat\u001b[39m.\u001b[39mcategories) \u001b[39mfor\u001b[39;00m n,col \u001b[39min\u001b[39;00m df1\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(col\u001b[39m.\u001b[39;49mcat\u001b[39m.\u001b[39mcategories) \u001b[39m>\u001b[39m \u001b[39m3\u001b[39m}\n\u001b[0;32m    195\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEmbedded columns:\u001b[39m\u001b[39m\"\u001b[39m, embedded_cols)\n\u001b[0;32m    197\u001b[0m embedded_col_names \u001b[39m=\u001b[39m embedded_cols\u001b[39m.\u001b[39mkeys()\n",
      "File \u001b[1;32mc:\\Users\\sarvz\\miniconda3\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "File \u001b[1;32mc:\\Users\\sarvz\\miniconda3\\lib\\site-packages\\pandas\\core\\accessor.py:182\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[1;34m(self, obj, cls)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    180\u001b[0m     \u001b[39m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessor\n\u001b[1;32m--> 182\u001b[0m accessor_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor(obj)\n\u001b[0;32m    183\u001b[0m \u001b[39m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39m# NDFrame\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[1;32mc:\\Users\\sarvz\\miniconda3\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py:2849\u001b[0m, in \u001b[0;36mCategoricalAccessor.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   2848\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 2849\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(data)\n\u001b[0;32m   2850\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mvalues\n\u001b[0;32m   2851\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mindex\n",
      "File \u001b[1;32mc:\\Users\\sarvz\\miniconda3\\lib\\site-packages\\pandas\\core\\arrays\\categorical.py:2858\u001b[0m, in \u001b[0;36mCategoricalAccessor._validate\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   2855\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m   2856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate\u001b[39m(data):\n\u001b[0;32m   2857\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_categorical_dtype(data\u001b[39m.\u001b[39mdtype):\n\u001b[1;32m-> 2858\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only use .cat accessor with a \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategory\u001b[39m\u001b[39m'\u001b[39m\u001b[39m dtype\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can only use .cat accessor with a 'category' dtype"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as torch_optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from datetime import datetime\n",
    "\n",
    "# UTILITIES\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "class ShelterOutcomeDataset(Dataset):\n",
    "    def __init__(self, X, Y, embedded_col_names):\n",
    "        X = X.copy()\n",
    "        self.X1 = X.loc[:,embedded_col_names].copy().values.astype(np.int64) #categorical columns\n",
    "        self.X2 = X.drop(columns=embedded_col_names).copy().values.astype(np.float32) #numerical columns\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
    "    \n",
    "\n",
    "# Model\n",
    "class ShelterOutcomeModel(nn.Module):\n",
    "    def __init__(self, embedding_sizes, n_cont):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(categories, size) for categories,size in embedding_sizes])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeddings) #length of all embeddings combined\n",
    "        self.n_emb, self.n_cont = n_emb, n_cont\n",
    "        self.lin1 = nn.Linear(self.n_emb + self.n_cont, 200)\n",
    "        self.lin2 = nn.Linear(200, 70)\n",
    "        self.lin3 = nn.Linear(70, 5)\n",
    "        self.bn1 = nn.BatchNorm1d(self.n_cont)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.bn3 = nn.BatchNorm1d(70)\n",
    "        self.emb_drop = nn.Dropout(0.6)\n",
    "        self.drops = nn.Dropout(0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeddings)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.emb_drop(x)\n",
    "        x2 = self.bn1(x_cont)\n",
    "        x = torch.cat([x, x2], 1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = self.drops(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.lin3(x)\n",
    "        return x\n",
    "    \n",
    "# Optimizer \n",
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim\n",
    "\n",
    "# Training function\n",
    "def train_model(model, optim, train_dl):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x1, x2, y in train_dl:\n",
    "        batch = y.shape[0]\n",
    "        output = model(x1, x2)\n",
    "        loss = F.cross_entropy(output, y)   \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "    return sum_loss/total\n",
    "\n",
    "# Evaluation function\n",
    "def val_loss(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x1, x2, y in valid_dl:\n",
    "        current_batch_size = y.shape[0]\n",
    "        out = model(x1, x2)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += current_batch_size*(loss.item())\n",
    "        total += current_batch_size\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        correct += (pred == y).float().sum().item()\n",
    "    print(\"valid loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))\n",
    "    return sum_loss/total, correct/total\n",
    "\n",
    "def train_loop(model, epochs, lr=0.01, wd=0.0):\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    for i in range(epochs): \n",
    "        loss = train_model(model, optim, train_dl)\n",
    "        print(\"training loss: \", loss)\n",
    "        val_loss(model, valid_dl)\n",
    "\n",
    "def main():\n",
    "\n",
    "    # read file to dataframe\n",
    "    df = pd.read_csv(\"diabetic_data.csv\")\n",
    "    # for readibility on id operations only\n",
    "    df['ID'] = df['encounter_id']\n",
    "    # divide 80/20 for training and testing\n",
    "    df1 = df[df.index % 5 != 0]  # Excludes every 5th row starting from 0\n",
    "    df2 = df[df.index % 5 == 0]  # Selects every 5th raw starting from 0\n",
    "    # remove data that is irrelevant\n",
    "    train_df = df1.drop(['encounter_id', 'weight', 'patient_nbr', 'discharge_disposition_id', 'admission_source_id',\n",
    "                         'payer_code', 'number_outpatient', 'number_emergency', 'number_inpatient', 'medical_specialty'], axis=1)\n",
    "\n",
    "    # DATA PREPROCESSING\n",
    "    Y = df1['readmitted']\n",
    "    test_df = df2\n",
    "    # Stacking train and test set so that they undergo the same preprocessing\n",
    "    df = train_df.append(test_df.drop(columns=['ID']))\n",
    "    # remove data that has null values\n",
    "    df.dropna(axis=0)\n",
    "\n",
    "    # label encoding\n",
    "    for col in df.columns:\n",
    "        if df.dtypes[col] == \"object\":\n",
    "            df[col] = df[col].fillna(\"NA\")\n",
    "        else:\n",
    "            df[col] = df[col].fillna(0)\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "    df.head()\n",
    "    # making all variables categorical\n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    # df.info()\n",
    "    # splitting back train and test\n",
    "    train_df = df[df.index % 5 != 0]\n",
    "    test_df = df[df.index % 5 == 0]\n",
    "\n",
    "    # check if shape[0] matches original\n",
    "    print(\"train shape: \", train_df.shape, \"orignal: \", df1.shape)\n",
    "    print(\"test shape: \", test_df.shape, \"original: \", df2.shape)\n",
    "\n",
    "    # Encoding Target\n",
    "    Y = LabelEncoder().fit_transform(Y)\n",
    "    # sanity check to see numbers match and matching with previous counter to create target dictionary\n",
    "    print(Counter(df1['readmitted']))\n",
    "    print(Counter(Y))\n",
    "    target_dict = {\n",
    "        'NO': 2,\n",
    "        '>30': 1,\n",
    "        '<30': 0,\n",
    "    }\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_df, Y, test_size=0.10, random_state=0)\n",
    "    X_train.head()\n",
    "\n",
    "    # Choosing columns for embedding\n",
    "    #categorical embedding for columns having more than three values\n",
    "    embedded_cols = {n: len(col.cat.categories) for n,col in train_df.items() if len(col.cat.categories) > 3}\n",
    "    print(\"Embedded columns:\", embedded_cols)\n",
    "\n",
    "    embedded_col_names = embedded_cols.keys()\n",
    "    len(train_df.columns) - len(embedded_cols) #number of numerical columns\n",
    "\n",
    "    # Determining size of embedding\n",
    "    embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]\n",
    "    print(embedding_sizes)\n",
    "    print(\"Embedding sizes:\", embedding_sizes)\n",
    "\n",
    "    train_df.to_csv(\"diabetic_data_training.csv\")\n",
    "    test_df.to_csv(\"diabetic_data_testing.csv\")\n",
    "\n",
    "    #creating train and valid datasets\n",
    "    train_ds = ShelterOutcomeDataset(X_train, y_train, embedded_col_names)\n",
    "    valid_ds = ShelterOutcomeDataset(X_val, y_val, embedded_col_names)\n",
    "\n",
    "    # creating model and sending it to gpu if possible\n",
    "    model = ShelterOutcomeModel(embedding_sizes, 1)\n",
    "    to_device(model, device)\n",
    "\n",
    "    # Training \n",
    "    batch_size = 1000\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    train_dl = DeviceDataLoader(train_dl, device)\n",
    "    valid_dl = DeviceDataLoader(valid_dl, device)\n",
    "\n",
    "    train_loop(model, epochs=8, lr=0.05, wd=0.00001)\n",
    "\n",
    "    # Test Output\n",
    "    test_ds = ShelterOutcomeDataset(test_df, np.zeros(len(test_df)), embedded_col_names)\n",
    "    test_dl = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for x1,x2,y in test_dl:\n",
    "            out = model(x1, x2)\n",
    "            prob = F.softmax(out, dim=1)\n",
    "            preds.append(prob)\n",
    "    final_probs = [item for sublist in preds for item in sublist]\n",
    "\n",
    "    sample_df=pd.from_dict(target_dict)\n",
    "    sample_df['NO']=[float(t[2]) for t in final_probs]\n",
    "    sample_df['>30']=[float(t[1]) for t in final_probs]\n",
    "    sample_df['<30']=[float(t[0]) for t in final_probs]\n",
    "    sample_df.head()\n",
    "    sample_df.to_csv('diabetic_trained_model_probabilties.csv', index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
