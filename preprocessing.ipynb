{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def fix_age(age) -> np.int8:\n",
    "    if age == '[0-10)':\n",
    "        return 5\n",
    "    elif age == '[10-20)':\n",
    "        return 15\n",
    "    elif age == '[20-30)':\n",
    "        return 25\n",
    "    elif age == '[30-40)':\n",
    "        return 35\n",
    "    elif age == '[40-50)':\n",
    "        return 45\n",
    "    elif age == '[50-60)':\n",
    "        return 55\n",
    "    elif age == '[60-70)':\n",
    "        return 65\n",
    "    elif age == '[70-80)':\n",
    "        return 75\n",
    "    elif age == '[80-90)':\n",
    "        return 85\n",
    "    elif age == '[90-100)':\n",
    "        return 95\n",
    "\n",
    "\n",
    "def fix_demographics(row) -> np.int8:\n",
    "    if row['gender'] == 'Female':\n",
    "        if row['race'] == 'Caucasian':\n",
    "            return 1\n",
    "        elif row['race'] == 'Asian':\n",
    "            return 2\n",
    "        elif row['race'] == ' AfricanAmerican':\n",
    "            return 3\n",
    "        elif row['race'] == 'Hispanic':\n",
    "            return 4\n",
    "        elif row['race'] == 'Other':\n",
    "            return 5\n",
    "        else:\n",
    "            return 6\n",
    "    elif row['gender'] == 'Male':\n",
    "        if row['race'] == 'Caucasian':\n",
    "            return 7\n",
    "        elif row['race'] == 'Asian':\n",
    "            return 8\n",
    "        elif row['race'] == ' AfricanAmerican':\n",
    "            return 9\n",
    "        elif row['race'] == 'Hispanic':\n",
    "            return 10\n",
    "        elif row['race'] == 'Other':\n",
    "            return 11\n",
    "        else:\n",
    "            return 12\n",
    "    else:\n",
    "        if row['race'] == 'Caucasian':\n",
    "            return 13\n",
    "        elif row['race'] == 'Asian':\n",
    "            return 14\n",
    "        elif row['race'] == ' AfricanAmerican':\n",
    "            return 15\n",
    "        elif row['race'] == 'Hispanic':\n",
    "            return 16\n",
    "        elif row['race'] == 'Other':\n",
    "            return 17\n",
    "        else:\n",
    "            return 18\n",
    "\n",
    "\n",
    "def data_normalization(item) -> np.int8:\n",
    "    # Values: '>200,' '>300','normal', and 'none' if not measured\n",
    "    if item == 'None' or item == 'No' or item == 'NO':\n",
    "        return 0\n",
    "    elif item == 'Yes' or item == 'Ch' or item == 'Steady' or item == '<30' or item == 'Norm':\n",
    "        return 1\n",
    "    elif item == '>30' or item == 'Up' or item == '>200' or item == '>7':\n",
    "        return 2\n",
    "    elif item == '>300' or item == 'Down' or item == '>8':\n",
    "        return 3\n",
    "    elif item == '?':\n",
    "        return None\n",
    "    else:\n",
    "        return item\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # read file to dataframe\n",
    "    df = pd.read_csv(\"diabetic_data.csv\")\n",
    "\n",
    "    # remove data that is irrelevant\n",
    "    df = df.drop(['encounter_id', 'weight', 'patient_nbr', 'discharge_disposition_id', 'admission_source_id',\n",
    "                'payer_code', 'number_outpatient', 'number_emergency', 'number_inpatient', 'medical_specialty'], axis=1)\n",
    "   \n",
    "    # prepare data for a Classification or Clustering machine learning \n",
    "    # algorithm by converting data to numeric or nominal format\n",
    "    df['age'] = df['age'].apply(fix_age)\n",
    "    df['demographics'] = df.apply(fix_demographics, axis=1)\n",
    "    df = df.applymap(lambda item: data_normalization(item))\n",
    "    # df['max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide,glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 'metformin-pioglitazone', 'change', 'diabetesMed', 'readmitted'] = df['max_glu_serum',\n",
    "\n",
    "    # df.info()\n",
    "    # df.dropna(axis=1, thresh=10)\n",
    "    # remove data that has null values\n",
    "    df  = df.drop(['gender', 'race'], axis=1)\n",
    "    df.dropna(axis=0)\n",
    "\n",
    "    # divide 80/20 for training and testing\n",
    "    df1 = df[df.index % 5 != 0]  # Excludes every 5th row starting from 0\n",
    "    df2 = df[df.index % 5 == 0]  # Selects every 5th raw starting from 0\n",
    "\n",
    "    \n",
    "    df1.to_csv(\"diabetic_data_training.csv\")\n",
    "    df2.to_csv(\"diabetic_data_testing.csv\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "tensor([1, 2, 3]) cpu\n",
      "tensor([1, 2, 3], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# x = torch.rand(5, 3)\n",
    "# print(x)\n",
    "# !nvidia-smi\n",
    "torch.cuda.is_available()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "# Count number of devices\n",
    "torch.cuda.device_count()\n",
    "\n",
    "# Create tensor (default on CPU)\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Tensor not on GPU\n",
    "print(tensor, tensor.device)\n",
    "\n",
    "# Move tensor to GPU (if available)\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "print(tensor_on_gpu)\n",
    "\n",
    "# If tensor is on GPU, can't transform it to NumPy (this will error)\n",
    "# tensor_on_gpu.numpy()\n",
    "\n",
    "# Instead, copy the tensor back to cpu\n",
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as torch_optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEVICE UTILITIES\n",
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "    \n",
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabeticPatientDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        # stuff\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return # stuff\n",
    "    \n",
    "\n",
    "# Model\n",
    "class DiabeticPatientModel(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self,):\n",
    "\n",
    "        return # stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (81412, 35) original:  (81412, 50)\n",
      "test shape:  (20354, 35) original:  (20354, 50)\n",
      "Counter({'NO': 43877, '>30': 28461, '<30': 9074})\n",
      "Counter({2: 43877, 1: 28461, 0: 9074})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# read file to dataframe\n",
    "df = pd.read_csv(\"diabetic_data.csv\")\n",
    "# for readibility on id operations only\n",
    "# df['ID'] = df['encounter_id']\n",
    "# divide 80/20 for training and testing\n",
    "df1 = df[df.index % 5 != 0]  # Excludes every 5th row starting from 0\n",
    "df2 = df[df.index % 5 == 0]  # Selects every 5th raw starting from 0\n",
    "# remove data that is irrelevant\n",
    "train_df = df1.drop(['encounter_id', 'weight', 'patient_nbr', 'discharge_disposition_id', 'admission_source_id',\n",
    "                        'payer_code', 'number_outpatient', 'number_emergency', 'number_inpatient', 'medical_specialty'], axis=1)\n",
    "test_df = df2.drop(['encounter_id', 'weight', 'patient_nbr', 'discharge_disposition_id', 'admission_source_id',\n",
    "                        'payer_code', 'number_outpatient', 'number_emergency', 'number_inpatient', 'medical_specialty'], axis=1)\n",
    "\n",
    "df=df.drop(['encounter_id', 'weight', 'patient_nbr', 'discharge_disposition_id', 'admission_source_id',\n",
    "                        'payer_code', 'number_outpatient', 'number_emergency', 'number_inpatient', 'medical_specialty'], axis=1)\n",
    "\n",
    "NUMERIC_COLUMNS = [ 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_diagnoses']\n",
    "# remove numeric columns\n",
    "df=df.drop(columns=NUMERIC_COLUMNS)\n",
    "# DATA PREPROCESSING\n",
    "Y = df1['readmitted']\n",
    "# Stacking train and test set so that they undergo the same preprocessing\n",
    "# df = train_df.append(test_df.drop(columns=['ID']))\n",
    "# remove data that has null values\n",
    "df.dropna(axis=0)\n",
    "\n",
    "# label encoding\n",
    "for col in df.columns:\n",
    "    # if df.dtypes[col] == \"object\":\n",
    "    #     df[col] = df[col].fillna(\"NA\")\n",
    "    # else:\n",
    "    #     df[col] = df[col].fillna(0)\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "df.head()\n",
    "# making all variables categorical\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "\n",
    "# df.info()\n",
    "# splitting back train and test\n",
    "train_df = df[df.index % 5 != 0]\n",
    "test_df = df[df.index % 5 == 0]\n",
    "\n",
    "# check if shape[0] matches original\n",
    "print(\"train shape: \", train_df.shape, \"original: \", df1.shape)\n",
    "print(\"test shape: \", test_df.shape, \"original: \", df2.shape)\n",
    "\n",
    "# Encoding Target\n",
    "Y = LabelEncoder().fit_transform(Y)\n",
    "# sanity check to see numbers match and matching with previous counter to create target dictionary\n",
    "print(Counter(df1['readmitted']))\n",
    "print(Counter(Y))\n",
    "target_dict = {\n",
    "    'NO': 2,\n",
    "    '>30': 1,\n",
    "    '<30': 0,\n",
    "}\n",
    "\n",
    "train_df.to_csv(\"diabetic_data_training.csv\")\n",
    "test_df.to_csv(\"diabetic_data_testing.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded columns: {'race': 6, 'age': 10, 'admission_type_id': 8, 'diag_1': 717, 'diag_2': 749, 'diag_3': 790, 'max_glu_serum': 4, 'A1Cresult': 4, 'metformin': 4, 'repaglinide': 4, 'nateglinide': 4, 'chlorpropamide': 4, 'glimepiride': 4, 'glipizide': 4, 'glyburide': 4, 'pioglitazone': 4, 'rosiglitazone': 4, 'acarbose': 4, 'miglitol': 4, 'insulin': 4, 'glyburide-metformin': 4}\n",
      "[(6, 3), (10, 5), (8, 4), (717, 50), (749, 50), (790, 50), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2)]\n",
      "Embedding sizes: [(6, 3), (10, 5), (8, 4), (717, 50), (749, 50), (790, 50), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2), (4, 2)]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_df, Y, test_size=0.10, random_state=0)\n",
    "X_train.head()\n",
    "\n",
    "# Choosing columns for embedding\n",
    "#categorical embedding for columns having more than three values\n",
    "embedded_cols = {n: len(col.cat.categories) for n,col in train_df.items() if len(col.cat.categories) > 3}\n",
    "print(\"Embedded columns:\", embedded_cols)\n",
    "\n",
    "embedded_col_names = embedded_cols.keys()\n",
    "len(train_df.columns) - len(embedded_cols) #number of numerical columns\n",
    "\n",
    "# Determining size of embedding\n",
    "embedding_sizes = [(n_categories, min(50, (n_categories+1)//2)) for _,n_categories in embedded_cols.items()]\n",
    "print(embedding_sizes)\n",
    "print(\"Embedding sizes:\", embedding_sizes)\n",
    "\n",
    "#creating train and valid datasets\n",
    "train_ds = ShelterOutcomeDataset(X_train, y_train, embedded_col_names)\n",
    "valid_ds = ShelterOutcomeDataset(X_val, y_val, embedded_col_names)\n",
    "\n",
    "# creating model and sending it to gpu if possible\n",
    "model = ShelterOutcomeModel(embedding_sizes, 1)\n",
    "to_device(model, device)\n",
    "\n",
    "# Training \n",
    "batch_size = 1000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size,shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size,shuffle=True)\n",
    "\n",
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer \n",
    "def get_optimizer(model, lr = 0.001, wd = 0.0):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optim = torch_optim.Adam(parameters, lr=lr, weight_decay=wd)\n",
    "    return optim\n",
    "\n",
    "# Training function\n",
    "def train_model(model, optim, train_dl):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x1, x2, y in train_dl:\n",
    "        batch = y.shape[0]\n",
    "        output = model(x1, x2)\n",
    "        loss = F.cross_entropy(output, y)   \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "    return sum_loss/total\n",
    "\n",
    "# Evaluation function\n",
    "def val_loss(model, valid_dl):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x1, x2, y in valid_dl:\n",
    "        current_batch_size = y.shape[0]\n",
    "        out = model(x1, x2)\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        sum_loss += current_batch_size*(loss.item())\n",
    "        total += current_batch_size\n",
    "        pred = torch.max(out, 1)[1]\n",
    "        correct += (pred == y).float().sum().item()\n",
    "    print(\"valid loss %.3f and accuracy %.3f\" % (sum_loss/total, correct/total))\n",
    "    return sum_loss/total, correct/total\n",
    "\n",
    "def train_loop(model, epochs, lr=0.01, wd=0.0):\n",
    "    optim = get_optimizer(model, lr = lr, wd = wd)\n",
    "    for i in range(epochs): \n",
    "        loss = train_model(model, optim, train_dl)\n",
    "        print(\"training loss: \", loss)\n",
    "        val_loss(model, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 14 elements not 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loop(model, epochs\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.05\u001b[39;49m, wd\u001b[39m=\u001b[39;49m\u001b[39m0.00001\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39m# Test Output\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_ds \u001b[39m=\u001b[39m ShelterOutcomeDataset(test_df, np\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(test_df)), embedded_col_names)\n",
      "Cell \u001b[1;32mIn[28], line 43\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, epochs, lr, wd)\u001b[0m\n\u001b[0;32m     41\u001b[0m optim \u001b[39m=\u001b[39m get_optimizer(model, lr \u001b[39m=\u001b[39m lr, wd \u001b[39m=\u001b[39m wd)\n\u001b[0;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs): \n\u001b[1;32m---> 43\u001b[0m     loss \u001b[39m=\u001b[39m train_model(model, optim, train_dl)\n\u001b[0;32m     44\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining loss: \u001b[39m\u001b[39m\"\u001b[39m, loss)\n\u001b[0;32m     45\u001b[0m     val_loss(model, valid_dl)\n",
      "Cell \u001b[1;32mIn[28], line 14\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optim, train_dl)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m x1, x2, y \u001b[39min\u001b[39;00m train_dl:\n\u001b[0;32m     13\u001b[0m     batch \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> 14\u001b[0m     output \u001b[39m=\u001b[39m model(x1, x2)\n\u001b[0;32m     15\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(output, y)   \n\u001b[0;32m     16\u001b[0m     optim\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\sarvz\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[20], line 36\u001b[0m, in \u001b[0;36mShelterOutcomeModel.forward\u001b[1;34m(self, x_cat, x_cont)\u001b[0m\n\u001b[0;32m     34\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(x, \u001b[39m1\u001b[39m)\n\u001b[0;32m     35\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_drop(x)\n\u001b[1;32m---> 36\u001b[0m x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(x_cont)\n\u001b[0;32m     37\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x, x2], \u001b[39m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin1(x))\n",
      "File \u001b[1;32mc:\\Users\\sarvz\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\sarvz\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\sarvz\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2452\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 14 elements not 1"
     ]
    }
   ],
   "source": [
    "train_loop(model, epochs=8, lr=0.05, wd=0.00001)\n",
    "\n",
    "# Test Output\n",
    "test_ds = DiabeticPatientDataset(test_df, np.zeros(len(test_df)), embedded_col_names)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for x1,x2,y in test_dl:\n",
    "        out = model(x1, x2)\n",
    "        prob = F.softmax(out, dim=1)\n",
    "        preds.append(prob)\n",
    "final_probs = [item for sublist in preds for item in sublist]\n",
    "\n",
    "sample_df=pd.from_dict(target_dict)\n",
    "sample_df['NO']=[float(t[2]) for t in final_probs]\n",
    "sample_df['>30']=[float(t[1]) for t in final_probs]\n",
    "sample_df['<30']=[float(t[0]) for t in final_probs]\n",
    "sample_df.head()\n",
    "sample_df.to_csv('diabetic_trained_model_probabilties.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
